# Network Pinpointer Demo Walkthrough

Interactive demonstration of Network Pinpointer's capabilities through real-world scenarios.

---

## Scenario 1: The Mysterious API Slowdown

### ğŸ­ The Situation
**Time**: Monday 9:00 AM
**Report**: "Our production API is slow. Users are complaining."
**Your mission**: Diagnose the issue using Network Pinpointer

### Step 1: Initial Health Check

```bash
$ ./pinpoint health
```

**Output**:
```
======================================================================
Network Health Status
======================================================================

Current State
Time: 2025-11-05 09:00:15
Devices: 10

âš ï¸ Health Score: 52% (POOR)

LJPW Coordinates
âš ï¸ Love         0.45  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  FAIR
âœ“ Justice      0.35  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  LOW
âš ï¸ Power        0.35  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  POOR
âœ“ Wisdom       0.75  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  GOOD

Baseline Comparison
Love       0.85 â†’ 0.45  (-47%)  â†“  SIGNIFICANT DEGRADATION
Justice    0.30 â†’ 0.35  (+17%)  â†‘
Power      0.80 â†’ 0.35  (-56%)  â†“  CRITICAL DEGRADATION
Wisdom     0.75 â†’ 0.75  (0%)    â†’  STABLE

Recent Alerts
  ğŸ”´ CRITICAL: Power decreased significantly (expected 0.80, got 0.35)
  ğŸŸ  HIGH: Love decreased significantly (expected 0.85, got 0.45)
```

### ğŸ” Analysis So Far

**What we see**:
- Health Score: 52% (POOR) - something is definitely wrong
- **Love â†“47%**: Connectivity/responsiveness has degraded significantly
- **Power â†“56%**: Network capacity is severely impacted
- **Justice stable**: Not a security/firewall issue
- **Wisdom stable**: DNS and routing intelligence intact

**Initial hypothesis**: This looks like a capacity or congestion problem, not a security block.

### Step 2: Run Slow Connection Recipe

```bash
$ ./pinpoint run slow_connection api.prod.example.com
```

**Output**:
```
======================================================================
Recipe: Slow Connection Diagnosis
======================================================================

Diagnose why a connection is slower than expected

Diagnostic Plan:
  1. Ping Latency Test (required)
     Measure basic latency and packet loss
  2. Path Analysis (required)
     Identify path complexity and routing issues
  3. Packet Capture Analysis (required)
     Deep dive into packet metadata
  4. Semantic Analysis (required)
     Map findings to LJPW dimensions

Focus Areas:
This recipe analyzes: Power, Love, Wisdom

Estimated time: 60s

Running diagnostics... â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%

Results:
  âœ“ Ping Latency Test: Average 145ms (baseline: 45ms)
  âš ï¸ Packet Loss: 8.5% (baseline: 0%)
  âœ“ Path Analysis: 18 hops (baseline: 12 hops)
  âš ï¸ TTL variance: High (route instability detected)

LJPW Semantic Analysis:
  Love:   0.45 (degraded - high latency)
  Power:  0.35 (critical - packet loss + congestion)
  Wisdom: 0.75 (good - routing visible, just suboptimal)

Pattern Match: "The Congestion Point"
  Severity: HIGH
  Description: Low Power, burst loss, time-dependent

  Likely Causes:
    â€¢ Link bandwidth saturation
    â€¢ Bottleneck at network device
    â€¢ Too much traffic for capacity
    â€¢ Peak usage overwhelming link

  Recommended Actions:
    âœ“ Check if this is peak usage time (it is - 9 AM)
    âœ“ Monitor bandwidth utilization
    âœ“ Consider scaling network capacity
    âœ“ Implement QoS for critical traffic
```

### ğŸ’¡ Diagnosis

**Root Cause**: Network congestion during peak hours
**Evidence**:
- Packet loss 8.5% (Power dimension low)
- Latency tripled (Love dimension degraded)
- Matches "The Congestion Point" pattern perfectly

**Recommended Fix**: Scale bandwidth or implement traffic shaping

---

## Scenario 2: Database Connection Failures

### ğŸ­ The Situation
**Time**: Tuesday 2:00 PM
**Report**: "Application can't connect to production database after firewall update"
**Your mission**: Identify what the firewall change broke

### Step 1: Compare Before/After States

```bash
# You captured baseline before the change (smart!)
$ ./pinpoint diff database-before-fw-update.json database-after-fw-update.json
```

**Output**:
```
======================================================================
Network State Comparison: Before FW Update â†’ After FW Update
======================================================================

Overall Assessment:
  CRITICAL: Major network state change detected
  Total Drift: 1.85

Dimension Changes:
  Love       0.92 â†’ 0.15  (-0.77, -84%)   â†“  [CRITICAL]
  Justice    0.25 â†’ 0.85  (+0.60, +240%)  â†‘  [MAJOR]
  Power      0.88 â†’ 0.85  (-0.03, -3%)    â†“  [MINOR]
  Wisdom     0.90 â†’ 0.88  (-0.02, -2%)    â†“  [NONE]

Major Changes:
  â€¢ Love decreased by 84% (CRITICAL)
  â€¢ Justice increased by 240% (MAJOR)
  â€¢ Power stable (only -3%)
  â€¢ Wisdom stable (only -2%)

Likely Causes:
  â€¢ Network over-secured (new firewall rules blocking traffic)
  â€¢ Firewall blocking legitimate database connections
  â€¢ ACL or security group misconfiguration
  â€¢ Port 5432 (PostgreSQL) may be blocked

Pattern Match: "The Over-Secured Network"
  High Justice (>0.7), Low Love (<0.4)
  Severity: HIGH

  Symptoms:
    âœ“ Firewall blocking legitimate traffic
    âœ“ Can't connect even though route exists
    âœ“ Recent security policy changes
```

### ğŸ” Investigation

```bash
$ ./pinpoint run cant_connect db.prod.internal
```

**Output**:
```
======================================================================
Recipe: Connection Failure Diagnosis
======================================================================

Results:
  âœ“ DNS Resolution: SUCCESS (db.prod.internal â†’ 10.0.5.23)
  âœ— Ping Reachability: TIMEOUT (0/5 packets received)
  âœ— Port 5432: FILTERED (firewall blocking)
  âœ“ Route exists: Path visible in traceroute
  âš ï¸ Semantic: High Justice blocking Love

Diagnosis:
  DNS works âœ“
  Route exists âœ“
  Host reachable âœ—
  Port accessible âœ—

  â†’ Firewall/ACL is blocking traffic

Pattern Match: "The Over-Secured Network"

Recommended Actions:
  1. Review firewall rules added in recent update
  2. Check for default-deny rules catching database traffic
  3. Verify port 5432 is allowed from application servers
  4. Test from same subnet to isolate firewall vs routing
```

### ğŸ’¡ Diagnosis

**Root Cause**: Firewall update blocked database port
**Evidence**:
- Justice â†‘240% (massive increase in policy enforcement)
- Love â†“84% (connectivity destroyed)
- Power/Wisdom stable (not a performance or DNS issue)
- Port 5432 shows FILTERED status

**Recommended Fix**: Add firewall rule allowing app servers â†’ database:5432

---

## Scenario 3: The Intermittent DNS Mystery

### ğŸ­ The Situation
**Time**: Wednesday 11:00 AM
**Report**: "DNS resolution works... sometimes. Very weird."
**Your mission**: Track down this flaky behavior

### Step 1: Run Intermittent Issues Recipe

```bash
$ ./pinpoint run intermittent nameserver.internal
```

**Output**:
```
======================================================================
Recipe: Intermittent Connection Issues
======================================================================

Running extended diagnostics (120 seconds)...

Results:
  Ping Test (100 packets):
    Success rate: 73% (73/100 received)
    Pattern: Periodic failures every ~30 seconds
    Latency when working: 5ms
    Latency variance: LOW when working, TIMEOUT when failing

  Route Stability (5 iterations):
    âœ“ Route stable (same path every time)
    âœ— Destination reachable: 60% success rate

  Packet Loss Pattern:
    Type: PERIODIC (not random)
    Interval: ~30 seconds
    Duration: ~5 seconds each failure

LJPW Semantic Analysis:
  Love:   0.55 (intermittent - varies 0.9 to 0.1)
  Justice: 0.30 (stable - not a policy issue)
  Power:  0.65 (moderate)
  Wisdom: 0.25 (CRITICAL - DNS intelligence failing)

Pattern Match: "The DNS Black Hole"
  Severity: CRITICAL
  Low Wisdom for DNS, periodic failures

  Likely Causes:
    â€¢ DNS server intermittently failing
    â€¢ Health check removing/adding server from pool
    â€¢ DNS server overloaded and timing out
    â€¢ Load balancer marking DNS server down periodically

  Symptoms Match:
    âœ“ Works sometimes but not always
    âœ“ Periodic pattern (not random)
    âœ“ When it works, it's fast
    âœ“ When it fails, complete timeout

  Recommended Actions:
    â€¢ Check DNS server health and logs
    â€¢ Review load balancer health check settings
    â€¢ Add redundant DNS servers
    â€¢ Adjust health check timeout/interval
```

### Step 2: Historical View

```bash
$ ./pinpoint history nameserver.internal --dimension wisdom --hours 2
```

**Output**:
```
Timeline: nameserver.internal (Wisdom dimension, last 2 hours)

1.00 â”¤
0.90 â”¤ â•­â”€â•® â•­â”€â•® â•­â”€â•® â•­â”€â•® â•­â”€â•®
0.80 â”¤â•­â•¯ â•°â•®â”‚ â•°â•®â”‚ â•°â•®â”‚ â•°â•®â”‚ â•°â•®
0.70 â”¤â”‚   â”‚â”‚  â”‚â”‚  â”‚â”‚  â”‚â”‚  â”‚
0.60 â”¤â”‚   â”‚â”‚  â”‚â”‚  â”‚â”‚  â”‚â”‚  â”‚
0.50 â”¤â”‚   â”‚â”‚  â”‚â”‚  â”‚â”‚  â”‚â”‚  â”‚
0.40 â”¤â”‚   â”‚â”‚  â”‚â”‚  â”‚â”‚  â”‚â”‚  â”‚
0.30 â”¤â”‚   â”‚â”‚  â”‚â”‚  â”‚â”‚  â”‚â”‚  â”‚
0.20 â”¤â”‚   â”‚â”‚  â”‚â”‚  â”‚â”‚  â”‚â”‚  â”‚
0.10 â”¤â•¯   â•°â•¯  â•°â•¯  â•°â•¯  â•°â•¯  â•°
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
     09:00  09:30  10:00 10:30 11:00

Statistics:
  Mean: 0.52
  Min: 0.05 (periodic drops)
  Max: 0.92 (when working)
  Pattern: PERIODIC sawtooth (drops every ~30 min)
  Trend: â†’ Stable pattern (repeating)

âš ï¸ ALERT: Periodic pattern detected
  Interval: ~30 minutes
  This suggests: Health check or maintenance cycle
```

### ğŸ’¡ Diagnosis

**Root Cause**: DNS server being periodically marked unhealthy by load balancer
**Evidence**:
- Wisdom â†“75% (DNS intelligence failing)
- Periodic pattern every 30 seconds (visible in timeline)
- When working, it works perfectly (rules out network issues)
- Pattern matches load balancer health check interval

**Recommended Fix**: Adjust load balancer health check settings or fix underlying DNS server health issue

---

## Scenario 4: Post-Deployment Validation

### ğŸ­ The Situation
**Time**: Thursday 4:00 PM
**Report**: "We're about to deploy new firewall rules. Help us validate they don't break anything."
**Your mission**: Capture before/after and verify no regression

### Step 1: Capture Baseline

```bash
$ ./pinpoint run baseline api.prod.example.com
```

**Output**:
```
======================================================================
Recipe: Network Performance Baseline
======================================================================

Establishing baseline for: api.prod.example.com

Tests running... â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%

Baseline Results:
  Love:    0.88 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘  EXCELLENT
  Justice: 0.32 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  LOW
  Power:   0.85 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  EXCELLENT
  Wisdom:  0.92 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘  EXCELLENT

  Health Score: 87% (GOOD)

Metrics:
  Latency: 42ms (p50), 58ms (p99)
  Packet Loss: 0.0%
  Jitter: 3ms
  TTL: 54 (12 hops)
  DNS Resolution: 8ms

Baseline saved to: ./baselines/api-prod-example-com-20251105-160000.json

Use this for future comparisons:
  ./pinpoint diff ./baselines/api-prod-example-com-20251105-160000.json current-state.json
```

### Step 2: Deploy Firewall Changes

```
# Firewall team deploys new rules...
```

### Step 3: Post-Deployment Check

```bash
$ ./pinpoint health api.prod.example.com > post-deploy.json
$ ./pinpoint diff ./baselines/api-prod-example-com-20251105-160000.json post-deploy.json
```

**Output**:
```
======================================================================
Network State Comparison: Pre-Deploy Baseline â†’ Post-Deploy
======================================================================

Overall Assessment:
  MINOR: Small changes detected, within acceptable range
  Total Drift: 0.18 (acceptable for config changes)

Dimension Changes:
  Love       0.88 â†’ 0.85  (-0.03, -3.4%)  â†“  [MINOR]
  Justice    0.32 â†’ 0.42  (+0.10, +31%)   â†‘  [MINOR]
  Power      0.85 â†’ 0.84  (-0.01, -1.2%)  â†“  [NONE]
  Wisdom     0.92 â†’ 0.91  (-0.01, -1.1%)  â†“  [NONE]

Analysis:
  â€¢ Justice increased by 31% (expected - new firewall rules)
  â€¢ Love decreased slightly by 3.4% (acceptable overhead)
  â€¢ Power and Wisdom essentially unchanged
  â€¢ No blocking detected
  â€¢ All services still reachable

âœ… VALIDATION PASSED

The firewall changes:
  âœ“ Added expected security (Justice â†‘)
  âœ“ Did NOT block legitimate traffic (Love still 0.85)
  âœ“ Did NOT impact performance (Power unchanged)
  âœ“ Did NOT break DNS/routing (Wisdom unchanged)

Recommendation: APPROVE deployment to production
```

### ğŸ’¡ Outcome

**Result**: Deployment validated successfully
**Evidence**:
- Justice increased as expected (new rules active)
- Love only slightly impacted (-3.4%, acceptable)
- No service disruption
- Total drift 0.18 (well within safe limits)

**Decision**: âœ… APPROVED for production rollout

---

## Scenario 5: Continuous Production Monitoring

### ğŸ­ The Situation
**Time**: Friday 8:00 AM
**Report**: "Set up 24/7 monitoring for critical services"
**Your mission**: Configure continuous monitoring with alerting

### Step 1: Create Configuration

```bash
$ ./pinpoint config create-example
$ vim ~/.network-pinpointer/config.yaml
```

**Configuration**:
```yaml
network_type: enterprise
monitoring_interval: 300  # Check every 5 minutes

targets:
  production_api:
    name: Production API
    host: api.prod.example.com
    ports: [443, 8080]
    baseline:
      love: 0.88
      justice: 0.32
      power: 0.85
      wisdom: 0.92
    alert_threshold: 0.15  # Alert if drift >15%
    critical: true

  production_db:
    name: Production Database
    host: db.prod.example.com
    ports: [5432]
    baseline:
      love: 0.95
      justice: 0.28
      power: 0.90
      wisdom: 0.88
    alert_threshold: 0.10  # Database is sensitive, lower threshold
    critical: true

  cdn_origin:
    name: CDN Origin
    host: cdn.prod.example.com
    ports: [443]
    baseline:
      love: 0.82
      justice: 0.40
      power: 0.78
      wisdom: 0.85
    alert_threshold: 0.20
    critical: false

monitoring:
  enabled: true
  interval: 300
  targets:
    - production_api
    - production_db
    - cdn_origin
  alert_methods:
    - slack
    - email

alert_slack_webhook: https://hooks.slack.com/services/YOUR/WEBHOOK/HERE
alert_email: oncall@example.com
```

### Step 2: Start Monitoring

```bash
$ ./pinpoint watch production_api production_db cdn_origin --interval 300
```

**Output**:
```
======================================================================
Network Watch Mode - Continuous Monitoring
======================================================================

Targets: 3
  â€¢ production_api (CRITICAL)
  â€¢ production_db (CRITICAL)
  â€¢ cdn_origin

Check Interval: 300 seconds (5 minutes)
Alert Methods: slack, email

Starting monitoring... Press Ctrl+C to stop

[08:00:00] Initial baseline check...
  âœ“ production_api: Health 87% (GOOD)
  âœ“ production_db: Health 92% (EXCELLENT)
  âœ“ cdn_origin: Health 81% (GOOD)

[08:05:00] Periodic check #1...
  âœ“ production_api: Health 86% (GOOD) [drift: 0.03]
  âœ“ production_db: Health 91% (EXCELLENT) [drift: 0.05]
  âœ“ cdn_origin: Health 80% (GOOD) [drift: 0.04]

[08:10:00] Periodic check #2...
  âœ“ production_api: Health 85% (GOOD) [drift: 0.08]
  âœ“ production_db: Health 90% (EXCELLENT) [drift: 0.07]
  âš ï¸ cdn_origin: Health 68% (FAIR) [drift: 0.18] â† Approaching threshold

[08:15:00] Periodic check #3...
  âœ“ production_api: Health 86% (GOOD) [drift: 0.05]
  âš ï¸ production_db: Health 78% (GOOD) [drift: 0.16] â† ALERT THRESHOLD EXCEEDED
  âš ï¸ cdn_origin: Health 65% (FAIR) [drift: 0.22] â† ALERT THRESHOLD EXCEEDED

ğŸš¨ ALERT TRIGGERED: production_db
  Previous Health: 92% (EXCELLENT)
  Current Health: 78% (GOOD)
  Drift: 0.16 (threshold: 0.10)

  Dimension Changes:
    Love:  0.95 â†’ 0.72 (-24%) â† SIGNIFICANT
    Power: 0.90 â†’ 0.75 (-17%) â† SIGNIFICANT

  Pattern: "The Congestion Point"
  Likely Cause: Database under heavy load or network congestion

  ğŸ“§ Email sent to: oncall@example.com
  ğŸ’¬ Slack alert posted to: #production-alerts

[08:15:05] Recommendations generated:
  1. Check database server CPU/memory
  2. Review active connections
  3. Check for long-running queries
  4. Monitor network bandwidth to database
```

### Step 3: View Historical Trends

```bash
$ ./pinpoint history production_db --hours 24
```

**Output**:
```
Timeline: production_db (Health Score, last 24 hours)

100% â”¤ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
 90% â”¤â•­â•¯                    â•°â”€â”€â•®
 80% â”¤â”‚                        â•°â”€â•®
 70% â”¤â”‚                          â”‚
 60% â”¤â”‚                          â•°
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
     Thu 08:00          Fri 08:00

Events:
  ğŸŸ¢ Thu 08:00-23:00: Healthy (92-95%)
  ğŸŸ¡ Fri 00:00-07:00: Slight degradation (85-90%)
  ğŸŸ  Fri 08:00-08:15: Significant degradation (78%) â† CURRENT

Analysis:
  â€¢ Pattern: Degradation started at midnight
  â€¢ Trend: Declining over 8 hours
  â€¢ Rate: -2% per hour
  â€¢ Predicted: Will reach POOR (60%) in ~9 hours if trend continues

Recommendations:
  ğŸ” Investigate what changed at midnight
  ğŸ“Š Review batch jobs or scheduled tasks
  âš¡ Take action soon to prevent further degradation
```

### ğŸ’¡ Monitoring Outcome

**What we detected**:
- Database degradation detected automatically
- Alerts sent to Slack and email within 15 seconds
- Root cause hint provided (congestion pattern)
- Historical context shows degradation timeline
- Predictive analysis warns of further decline

**Response time**: From degradation to alert = 5 minutes (one check interval)

---

## Key Takeaways from Demo

### 1. **LJPW Reveals Intent**
Traditional tools say "8.5% packet loss" - Network Pinpointer says "You have a congestion problem during peak hours"

### 2. **Patterns Accelerate Diagnosis**
Instead of manual correlation, pattern matching instantly identifies "The Over-Secured Network" or "The DNS Black Hole"

### 3. **Before/After Comparison is Gold**
Diff mode turns "something changed" into "Justice increased 240%, likely new firewall rule blocking port 5432"

### 4. **Historical Context Matters**
Timeline showing periodic drops every 30 minutes immediately suggests load balancer health checks

### 5. **Semantic Dimensions Work**
- **Love â†“** = connectivity problem
- **Justice â†‘** = new policies/security
- **Power â†“** = capacity/performance issue
- **Wisdom â†“** = DNS/routing/visibility problem

### 6. **Recipes Save Time**
Instead of remembering which tests to run, `./pinpoint run cant_connect` runs the exact right diagnostic workflow

---

## Command Reference from Demo

### Quick Diagnostics
```bash
./pinpoint health                              # Overall health
./pinpoint quick-check api.example.com         # 30-second assessment
./pinpoint patterns                            # Known issue patterns
```

### Diagnostic Recipes
```bash
./pinpoint run slow_connection <host>          # Why is this slow?
./pinpoint run cant_connect <host>             # Why can't I connect?
./pinpoint run intermittent <host>             # Why is it flaky?
./pinpoint run baseline <host>                 # Capture healthy state
```

### Comparison & History
```bash
./pinpoint diff before.json after.json         # What changed?
./pinpoint history <host> --hours 24           # Show last 24 hours
./pinpoint history <host> --dimension love     # Track specific dimension
```

### Continuous Monitoring
```bash
./pinpoint config create-example               # Create config file
./pinpoint watch host1 host2 --interval 300    # Monitor continuously
```

### Export & Documentation
```bash
./pinpoint health > results.json
./pinpoint export results.json -f html         # Share with stakeholders
```

---

## Real-World Results

These scenarios are based on actual network issues that LJPW semantic analysis excels at diagnosing:

1. **Congestion**: Power â†“ + Love â†“ + time-dependent
2. **Firewall blocks**: Justice â†‘â†‘ + Love â†“â†“
3. **DNS issues**: Wisdom â†“â†“
4. **Route flapping**: Justice variance
5. **MTU problems**: Good Love + Bad Power for large packets

The LJPW framework transforms raw metrics into **semantic understanding** of what the network is trying to do and what's preventing it.

---

## Try It Yourself

Want to run these scenarios?

```bash
# Clone the repository
git clone https://github.com/BruinGrowly/Network-Pinpointer.git
cd Network-Pinpointer

# Start with interactive mode (great for learning)
./pinpoint interactive

# Or jump into a quick health check
./pinpoint quick-check 8.8.8.8

# View all available recipes
./pinpoint recipes

# Learn about LJPW dimensions
./pinpoint explain ljpw
```

---

**Network Pinpointer**: Because networks aren't just pipes - they're **communication systems with semantic meaning**.

The LJPW framework reveals that meaning. ğŸŒâœ¨
